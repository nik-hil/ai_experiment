{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan), tensor(nan))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def init():\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512)\n",
    "    return x,a\n",
    "\n",
    "x,a = init()\n",
    "\n",
    "for i in range(100): \n",
    "    x = a @ x\n",
    "\n",
    "x.std(), x.mean() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix calculation is simply blowing and pytorch is not able to keep up the calculation. it is giving us NaN.\n",
    "\n",
    "Let's findout iteration count after which the x.std() is blown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,a = init()\n",
    "for i in range(100): \n",
    "    x = a @ x\n",
    "    if torch.isnan(x.std()):\n",
    "        break\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets keep activation low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,a = init()\n",
    "a *= 0.01\n",
    "for i in range(100): \n",
    "    x = a @ x\n",
    "\n",
    "x.std(), x.mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the multiplications we want to have std of 1 & mean of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.013873589411377906 1.0052018302679062\n",
      "-0.048302864631841656 0.8950763777035272\n",
      "0.012876306110993028 0.9755765050308837\n"
     ]
    }
   ],
   "source": [
    "size = 512\n",
    "cnt = 100\n",
    "def check():\n",
    "    mean,sqr = 0.,0.\n",
    "    for i in range(cnt):\n",
    "        x = torch.randn(size)\n",
    "        a = torch.randn(size, size)\n",
    "        y = a @ x\n",
    "        mean += y.mean().item()\n",
    "        sqr  += y.pow(2).mean().item()\n",
    "    print(mean/cnt,sqr/(cnt*size)) #mean & std\n",
    "\n",
    "\n",
    "check()\n",
    "size = 1\n",
    "cnt = 1000\n",
    "check()\n",
    "size = 2\n",
    "cnt = 1000\n",
    "check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is a link between the number of tensor shape and final mean, std we are calculating.\n",
    "`x` & `a` both have a uniform distribution. And if we keep multiplying `x & a` then accumulated avg mean & avg std is close to 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like basis for [Kaiming initialization](https://arxiv.org/abs/1502.01852)\n",
    "\n",
    "### Yes our initialization will affect our training of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
