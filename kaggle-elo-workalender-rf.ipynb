{
  "cells": [
    {
      "metadata": {
        "_uuid": "908956b65b7871816988c588b27ac4ba8b85edfe"
      },
      "cell_type": "markdown",
      "source": "### This heavily borrows from Chau Ngoc Huynh's  https://www.kaggle.com/chauhuynh/my-first-kernel-3-699. I try to create some features using *workalendar*, which was suggested on Bojan Tunguz's post (https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/74052) by Kjetil Åmdal-Sævik. \n\n### As a first cursory effort, I've created new features in the train and test sets based on the number of working days (in the Brazilian calendar) between the first_active_month and the 8 major national holidays.  "
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport datetime\nfrom datetime import date, datetime\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nimport warnings\n\nimport workalendar\nfrom workalendar.america import Brazil\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(4590)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\ndf_hist_trans = pd.read_csv('../input/historical_transactions.csv')\ndf_new_merchant_trans = pd.read_csv('../input/new_merchant_transactions.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "520b71064293a20e0f2a379dad0acc274374a3c4"
      },
      "cell_type": "code",
      "source": "def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "44198b73053869f8dfc083204c592fe9193f239c"
      },
      "cell_type": "code",
      "source": "df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)\ndf_hist_trans = reduce_mem_usage(df_hist_trans)\ndf_new_merchant_trans = reduce_mem_usage(df_new_merchant_trans)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71f89a3b8a93b2f2feb2cd0a45f860cde33687be"
      },
      "cell_type": "code",
      "source": "for df in [df_hist_trans,df_new_merchant_trans]:\n    df['category_2'].fillna(1.0,inplace=True)\n    df['category_3'].fillna('A',inplace=True)\n    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dda90662d05e22310dd713df106ea07f4b8bccfc"
      },
      "cell_type": "code",
      "source": "def get_new_columns(name,aggs):\n    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a8a999a7af28d3746de650cd9f48b4b24037d73"
      },
      "cell_type": "code",
      "source": "cal = Brazil()\n# for yr in [2011,2012,2013,2014,2015,2016,2017]:\n#     print(yr,cal.holidays(yr))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eff94dfde04e78f7e6054194896f57c117bcbfe9"
      },
      "cell_type": "code",
      "source": "cal.holidays(2013)[1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b72764c2f70d05fa46a858a1215424b9d638b3de"
      },
      "cell_type": "markdown",
      "source": "### As a first effort, for every year, we want to calculate the number of working days between the purchase date and the 8 major holidays -- \n* New years day -- (year,1,1) \n* Tiradentes day -- (year,4,21)\n* Labour day-- (year,5,1)\n* Independence day -- (year,9,7)\n* Our lady of aparecida day -- (year,10,12)\n* All souls day -- (year,11,2)\n* Republic day -- (year,11,15)\n* Christmas day (year,12,25)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c91d3b9e9dbaff01962b0facbace75705a9ce18"
      },
      "cell_type": "code",
      "source": "for df in [df_hist_trans,df_new_merchant_trans]:\n    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n#     df['date'] = df['purchase_date'].dt.date\n    df['year'] = df['purchase_date'].dt.year\n    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n    df['month'] = df['purchase_date'].dt.month\n    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n    df['hour'] = df['purchase_date'].dt.hour\n    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n    df['month_diff'] = ((datetime.today() - df['purchase_date']).dt.days)//30\n    df['month_diff'] += df['month_lag']\n    # These are the 8 added features, calculating the no of working days between the date of purchase and each of the 8 standard Brailian holidays\n\n#     df['day_diff1'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(x.year)[0][0])) # have to make this less clunky, write a function\n#     df['day_diff2'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(x.year)[1][0]))\n#     df['day_diff3'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(x.year)[2][0]))\n#     df['day_diff4'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(x.year)[3][0]))\n#     df['day_diff5'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(x.year)[4][0]))\n#     df['day_diff6'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(x.year)[5][0]))\n#     df['day_diff7'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(x.year)[6][0]))\n#     df['day_diff8'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(x.year)[7][0]))\n\n#     df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n#     df['date'] = df['purchase_date'].dt.date\n#     df['year'] = df['purchase_date'].dt.year\n#     df['weekofyear'] = df['purchase_date'].dt.weekofyear\n#     df['month'] = df['purchase_date'].dt.month\n#     df['dayofweek'] = df['purchase_date'].dt.dayofweek\n#     df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n#     df['hour'] = df['purchase_date'].dt.hour\n#     df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n#     df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n#     df['month_diff'] = ((datetime.today() - df['purchase_date']).dt.days)//30\n#     df['month_diff'] += df['month_lag']\n#     df['day_diff1'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(x.year)[0][0]))\n#                                        #cal.get_working_days_delta(df['date'],cal.holidays(2018)[0][0]) #df['date'] - cal.holidays(2018)[0][0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddf1d5bb0ade2b22b0f072c208c1506ea64503ea"
      },
      "cell_type": "code",
      "source": "aggs = {}\nfor col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n    aggs[col] = ['nunique']\n\naggs['purchase_amount'] = ['sum','max','min','mean','var']\naggs['installments'] = ['sum','max','min','mean','var']\naggs['purchase_date'] = ['max','min']\naggs['month_lag'] = ['max','min','mean','var']\naggs['month_diff'] = ['mean']\naggs['authorized_flag'] = ['sum', 'mean']\naggs['weekend'] = ['sum', 'mean']\naggs['category_1'] = ['sum', 'mean']\naggs['card_id'] = ['size']\n\nfor col in ['category_2','category_3']:\n    df_hist_trans[col+'_mean'] = df_hist_trans.groupby([col])['purchase_amount'].transform('mean')\n    aggs[col+'_mean'] = ['mean']    \n\nnew_columns = get_new_columns('hist',aggs)\ndf_hist_trans_group = df_hist_trans.groupby('card_id').agg(aggs)\ndf_hist_trans_group.columns = new_columns\ndf_hist_trans_group.reset_index(drop=False,inplace=True)\ndf_hist_trans_group['hist_purchase_date_diff'] = (df_hist_trans_group['hist_purchase_date_max'] - df_hist_trans_group['hist_purchase_date_min']).dt.days\ndf_hist_trans_group['hist_purchase_date_average'] = df_hist_trans_group['hist_purchase_date_diff']/df_hist_trans_group['hist_card_id_size']\ndf_hist_trans_group['hist_purchase_date_uptonow'] = (datetime.today() - df_hist_trans_group['hist_purchase_date_max']).dt.days\ndf_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\ndf_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n\ndel df_hist_trans_group;\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62050fec26b49ffff3feed6c845caafd6f06464a"
      },
      "cell_type": "code",
      "source": "df_new_merchant_trans.head().T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "4820ed16eecd539377a0ecc88aa32b0eac83de5e"
      },
      "cell_type": "code",
      "source": "df_train.head().T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7f5625db40db4395374991124fb796c9decd60b"
      },
      "cell_type": "code",
      "source": "aggs = {}\nfor col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n    aggs[col] = ['nunique']\naggs['purchase_amount'] = ['sum','max','min','mean','var']\naggs['installments'] = ['sum','max','min','mean','var']\naggs['purchase_date'] = ['max','min']\naggs['month_lag'] = ['max','min','mean','var']\naggs['month_diff'] = ['mean']\naggs['weekend'] = ['sum', 'mean']\naggs['category_1'] = ['sum', 'mean']\naggs['card_id'] = ['size']\n\nfor col in ['category_2','category_3']:\n    df_new_merchant_trans[col+'_mean'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('mean')\n    aggs[col+'_mean'] = ['mean']\n    \nnew_columns = get_new_columns('new_hist',aggs)\ndf_hist_trans_group = df_new_merchant_trans.groupby('card_id').agg(aggs)\ndf_hist_trans_group.columns = new_columns\ndf_hist_trans_group.reset_index(drop=False,inplace=True)\ndf_hist_trans_group['new_hist_purchase_date_diff'] = (df_hist_trans_group['new_hist_purchase_date_max'] - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\ndf_hist_trans_group['new_hist_purchase_date_average'] = df_hist_trans_group['new_hist_purchase_date_diff']/df_hist_trans_group['new_hist_card_id_size']\ndf_hist_trans_group['new_hist_purchase_date_uptonow'] = (datetime.today() - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\ndf_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\ndf_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n\ndel df_hist_trans_group;\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab49c5936c182ba280c2b06264ba38a58e7fa335"
      },
      "cell_type": "code",
      "source": "df_train.head().T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a075cc90ab1322829e4fad3ff39fce307c5db93c"
      },
      "cell_type": "code",
      "source": "del df_hist_trans;\ngc.collect()\n\ndel df_new_merchant_trans;\ngc.collect()\n\ndf_train.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f3182aeac0c3bf7a061a1b9e25e859f25fee9b5"
      },
      "cell_type": "code",
      "source": "df_train['outliers'] = 0\ndf_train.loc[df_train['target'] < -30, 'outliers'] = 1\ndf_train['outliers'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9f61512c195c66a75dfe220072c5b2d860b78a3"
      },
      "cell_type": "code",
      "source": "# Dealing with the one nan in df_test.first_active_month a bit arbitrarily for now\ndf_test.loc[df_test['first_active_month'].isna(),'first_active_month'] = df_test.iloc[11577]['first_active_month']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "61d6840e9bd87a5c14ea06d1f8ae567c307cdb33"
      },
      "cell_type": "code",
      "source": "from datetime import timedelta\ndef get_working_days_delta(begin, end):\n    '''\n    Get working days between two dates\n    '''\n    sign = 1\n    if begin > end:\n        begin, end = end, begin\n        sign = -1\n    days = 0\n    temp_day = begin\n    while temp_day <= end:\n        if cal.is_working_day(temp_day):\n            days += 1\n        temp_day = temp_day + timedelta(days=1)\n    return days * sign",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce2082fc1fb0e3f8f7d27fc166aa7a8351b65504"
      },
      "cell_type": "code",
      "source": "for df in [df_train,df_test]:\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.today() - df['first_active_month']).dt.days\n    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n                     'new_hist_purchase_date_min']:\n        df[f] = df[f].astype(np.int64) * 1e-9\n    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n    \n    df['date'] = df['first_active_month'].dt.date\n    \n     # These are the 8 added features, calculating the no of working days between the first active month and each of the 8 standard Brailian holidays\n        \n    df['day_diff1'] = df['date'].apply(lambda x: get_working_days_delta(x,cal.holidays(int(x.year))[0][0])) # have to make this less clunky, write a function\n    df['day_diff2'] = df['date'].apply(lambda x: get_working_days_delta(x,cal.holidays(int(x.year))[1][0]))\n    df['day_diff3'] = df['date'].apply(lambda x: get_working_days_delta(x,cal.holidays(int(x.year))[2][0]))\n    df['day_diff4'] = df['date'].apply(lambda x: get_working_days_delta(x,cal.holidays(int(x.year))[3][0]))\n    df['day_diff5'] = df['date'].apply(lambda x: get_working_days_delta(x,cal.holidays(int(x.year))[4][0]))\n    df['day_diff6'] = df['date'].apply(lambda x: get_working_days_delta(x,cal.holidays(int(x.year))[5][0]))\n    df['day_diff7'] = df['date'].apply(lambda x: get_working_days_delta(x,cal.holidays(int(x.year))[6][0]))\n    df['day_diff8'] = df['date'].apply(lambda x: get_working_days_delta(x,cal.holidays(int(x.year))[7][0]))\n    \n    df.drop(['date'],axis=1,inplace=True)\n    \nfor f in ['feature_1','feature_2','feature_3']:\n    order_label = df_train.groupby([f])['outliers'].mean()\n    df_train[f] = df_train[f].map(order_label)\n    df_test[f] = df_test[f].map(order_label)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10de5d846a12cdc85670cce7670238f4722043d6"
      },
      "cell_type": "code",
      "source": "df_trainin.head().T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a93c5976d3b395ba8ff0d1002b8075be3e914c54"
      },
      "cell_type": "code",
      "source": "df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d4a203d331fb6bf3868cad026d279467f2db8328"
      },
      "cell_type": "code",
      "source": "from IPython.lib.deepreload import reload as dreload\nimport PIL, os, numpy as np, math, collections, threading, json, bcolz, random, scipy, cv2\nimport pandas as pd, pickle, sys, itertools, string, sys, re, datetime, time, shutil, copy\nimport seaborn as sns, matplotlib\nimport IPython, graphviz, sklearn_pandas, sklearn, warnings, pdb\nimport contextlib\nfrom abc import abstractmethod\nfrom glob import glob, iglob\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\nfrom itertools import chain\nfrom functools import partial\nfrom collections import Iterable, Counter, OrderedDict\nfrom isoweek import Week\nfrom pandas_summary import DataFrameSummary\nfrom IPython.lib.display import FileLink\nfrom PIL import Image, ImageEnhance, ImageOps\nfrom sklearn import metrics, ensemble, preprocessing\nfrom operator import itemgetter, attrgetter\nfrom pathlib import Path\nfrom distutils.version import LooseVersion\n\nfrom matplotlib import pyplot as plt, rcParams, animation\nfrom ipywidgets import interact, interactive, fixed, widgets\nmatplotlib.rc('animation', html='html5')\nnp.set_printoptions(precision=5, linewidth=110, suppress=True)\n\nimport workalendar\nfrom workalendar.america import Brazil\n\nfrom ipykernel.kernelapp import IPKernelApp\ndef in_notebook(): return IPKernelApp.initialized()\n\ndef in_ipynb():\n    try:\n        cls = get_ipython().__class__.__name__\n        return cls == 'ZMQInteractiveShell'\n    except NameError:\n        return False\n\nimport tqdm as tq\nfrom tqdm import tqdm_notebook, tnrange\n\ndef clear_tqdm():\n    inst = getattr(tq.tqdm, '_instances', None)\n    if not inst: return\n    try:\n        for i in range(len(inst)): inst.pop().close()\n    except Exception:\n        pass\n\nif in_notebook():\n    def tqdm(*args, **kwargs):\n        clear_tqdm()\n        return tq.tqdm(*args, file=sys.stdout, **kwargs)\n    def trange(*args, **kwargs):\n        clear_tqdm()\n        return tq.trange(*args, file=sys.stdout, **kwargs)\nelse:\n    from tqdm import tqdm, trange\n    tnrange=trange\n    tqdm_notebook=tqdm\n\n\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\nfrom sklearn.ensemble import forest\nfrom sklearn.tree import export_graphviz\n\n\ndef set_plot_sizes(sml, med, big):\n    plt.rc('font', size=sml)          # controls default text sizes\n    plt.rc('axes', titlesize=sml)     # fontsize of the axes title\n    plt.rc('axes', labelsize=med)    # fontsize of the x and y labels\n    plt.rc('xtick', labelsize=sml)    # fontsize of the tick labels\n    plt.rc('ytick', labelsize=sml)    # fontsize of the tick labels\n    plt.rc('legend', fontsize=sml)    # legend fontsize\n    plt.rc('figure', titlesize=big)  # fontsize of the figure title\n\ndef parallel_trees(m, fn, n_jobs=8):\n        return list(ProcessPoolExecutor(n_jobs).map(fn, m.estimators_))\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\"\n    \"\"\"\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                      special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))\n\ndef combine_date(years, months=1, days=1, weeks=None, hours=None, minutes=None,\n              seconds=None, milliseconds=None, microseconds=None, nanoseconds=None):\n    years = np.asarray(years) - 1970\n    months = np.asarray(months) - 1\n    days = np.asarray(days) - 1\n    types = ('<M8[Y]', '<m8[M]', '<m8[D]', '<m8[W]', '<m8[h]',\n             '<m8[m]', '<m8[s]', '<m8[ms]', '<m8[us]', '<m8[ns]')\n    vals = (years, months, days, weeks, hours, minutes, seconds,\n            milliseconds, microseconds, nanoseconds)\n    return sum(np.asarray(v, dtype=t) for t, v in zip(types, vals)\n               if v is not None)\n\ndef get_sample(df,n):\n    \"\"\"\n    \"\"\"\n    idxs = sorted(np.random.permutation(len(df))[:n])\n    return df.iloc[idxs].copy()\n\ndef add_datepart(df, fldname, drop=True, time=False, errors=\"raise\"):\t\n    \"\"\"\n    \"\"\"\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n    if drop: df.drop(fldname, axis=1, inplace=True)\n\ndef is_date(x): return np.issubdtype(x.dtype, np.datetime64)\n\ndef train_cats(df):\n    \"\"\"\n    \"\"\"\n    for n,c in df.items():\n        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n\ndef apply_cats(df, trn):\n    \"\"\"\n    \"\"\"\n    for n,c in df.items():\n        if (n in trn.columns) and (trn[n].dtype.name=='category'):\n            df[n] = c.astype('category').cat.as_ordered()\n            df[n].cat.set_categories(trn[n].cat.categories, ordered=True, inplace=True)\n\ndef fix_missing(df, col, name, na_dict):\n    \"\"\" \n    \"\"\"\n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict\n\ndef numericalize(df, col, name, max_n_cat):\n    \"\"\" \n    \"\"\"\n    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = col.cat.codes+1\n\ndef scale_vars(df, mapper):\n    warnings.filterwarnings('ignore', category=sklearn.exceptions.DataConversionWarning)\n    if mapper is None:\n        map_f = [([n],StandardScaler()) for n in df.columns if is_numeric_dtype(df[n])]\n        mapper = DataFrameMapper(map_f).fit(df)\n    df[mapper.transformed_names_] = mapper.transform(df)\n    return mapper\n\ndef proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n    \"\"\" \n    \"\"\"\n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    else: df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = df[y_fld].cat.codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\ndef set_rf_samples(n):\n    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n    n random rows.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n))\n\ndef reset_rf_samples():\n    \"\"\" Undoes the changes produced by set_rf_samples.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n_samples))\n\ndef get_nn_mappers(df, cat_vars, contin_vars):\n    # Replace nulls with 0 for continuous, \"\" for categorical.\n    for v in contin_vars: df[v] = df[v].fillna(df[v].max()+100,)\n    for v in cat_vars: df[v].fillna('#NA#', inplace=True)\n\n    # list of tuples, containing variable and instance of a transformer for that variable\n    # for categoricals, use LabelEncoder to map to integers. For continuous, standardize\n    cat_maps = [(o, LabelEncoder()) for o in cat_vars]\n    contin_maps = [([o], StandardScaler()) for o in contin_vars]\n    return DataFrameMapper(cat_maps).fit(df), DataFrameMapper(contin_maps).fit(df)",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ef42c06cfe01ee949fa6eb0127832eb012565e2"
      },
      "cell_type": "code",
      "source": "df_trn, y, nas, mapper = proc_df(df_train, target, do_scale=True)\nn=len(df_trn)\ndf_test, _, nas, mapper = proc_df(test, target, do_scale=True, \n                                  mapper=mapper, na_dict=nas)\nn",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'[-0.8203  0.3928  0.688  ...  0.0935 -4.676  -1.859 ] not in index'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8586868724e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m df_test, _, nas, mapper = proc_df(test, target, do_scale=True, \n\u001b[1;32m      4\u001b[0m                                   mapper=mapper, na_dict=nas)\n\u001b[1;32m      5\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-1fe0de81d2cf>\u001b[0m in \u001b[0;36mproc_df\u001b[0;34m(df, y_fld, skip_flds, ignore_flds, do_scale, na_dict, preproc_fn, max_n_cat, subset, mapper)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_fld\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_numeric_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_fld\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_fld\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_fld\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_fld\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mskip_flds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_fld\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2726\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '[-0.8203  0.3928  0.688  ...  0.0935 -4.676  -1.859 ] not in index'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "220c9cd9ca49616827e1ef8195e8a532510b2e68"
      },
      "cell_type": "code",
      "source": "def split_vals(a,n): \n    return a[:n].copy(), a[n:].copy()\n\n\ny=target\nn_valid = int(len(df_train) * .25)\nn_trn = len(df_train)-n_valid\n# raw_train, raw_valid = split_vals(df_raw, n_trn)\nX_train, X_valid = split_vals(df_train, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "((151438, 94), (151438,), (50479, 94))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c8b96f1b6987a6d25bb56ba4fab45444375cf89"
      },
      "cell_type": "code",
      "source": "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n    \nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn import metrics",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc96b749e90a02574c5fda86787fffdf502a7021"
      },
      "cell_type": "code",
      "source": "m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22e37a2059532f9b6f2955f1f1aa93404021b9fb"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92110a41b48cb0fb198bd70ec78066c2414ff8a4"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc88bfc75c74d50c7a6e2bad21dadbbcd6d4c70f"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f553bbceb805db0a2c4d0e8329f5cddfaa3247ab"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31dbc681b727d0922a4477332a339c73efbf31ad"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "456fa8714e8dd307884fada8285ce50e2d91e31b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab129217afce74c5d3ab564aa2201d0b0033c693"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c4f20f27679889542acfd60d1f1ac381b201ac43"
      },
      "cell_type": "code",
      "source": "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\ntarget = df_train['target']\ndel df_train['target']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9bbc95244978b519d94131907b547c2b6c94191"
      },
      "cell_type": "code",
      "source": "param = {'num_leaves': 31,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"nthread\": 4,\n         \"random_state\": 4590}\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\noof = np.zeros(len(df_train))\npredictions = np.zeros(len(df_test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train,df_train['outliers'].values)):\n    print(\"fold {}\".format(fold_))\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][df_train_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(df_train.iloc[val_idx][df_train_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n    oof[val_idx] = clf.predict(df_train.iloc[val_idx][df_train_columns], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = df_train_columns\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(df_test[df_train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n\nnp.sqrt(mean_squared_error(oof, target))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40b64481054fa71e692829c7039eccceb31b77fe"
      },
      "cell_type": "code",
      "source": "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:500].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n            y=\"Feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\n# plt.savefig('lgbm_importances.png')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5aa9a8cbe586424712f418eb34182910ed82bcb5"
      },
      "cell_type": "markdown",
      "source": "### So most of the newly created features don't rank particularly high as far as feature importances go, but I will continue to work on this... "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "355e9c24949b8e5d677fe5a2f117228c3310dab6"
      },
      "cell_type": "code",
      "source": "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "58c9a5445698e42dfbd9548695290487a2ce171a"
      },
      "cell_type": "markdown",
      "source": "### Haven't had time to think deeply about how *workalendar*  might be used but there is definitely potential, and I hope this starts a discussion"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4a55780edb95483bfac0d8119ea4d344a8461ff"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}